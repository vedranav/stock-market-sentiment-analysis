{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stock Market Sentiment Analysis\n**Goals**:\n- Train a model that predicts sentiment towards a stock described in a social media post\n- Analyze which words are related to a positive and which to a negative sentiment\n- Export the model for production\n\n**Data**:\n- Data for this project is obtained from Kaggle: https://www.kaggle.com/datasets/yash612/stockmarket-sentiment-dataset","metadata":{}},{"cell_type":"markdown","source":"## Import libraries","metadata":{"_uuid":"036dbb9f-f6f3-4763-b528-b4c331c35f96","_cell_guid":"d1549623-7b3e-4e3a-b3e7-04e19050e64e","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport timeit\nimport nltk\nimport string\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"_uuid":"c21d5b20-08d3-4989-9ce4-97ad3b8bb1b5","_cell_guid":"73485142-73b7-47d4-9d24-e85b24faf326","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:42.422582Z","iopub.execute_input":"2023-09-18T15:52:42.423096Z","iopub.status.idle":"2023-09-18T15:52:43.573095Z","shell.execute_reply.started":"2023-09-18T15:52:42.423051Z","shell.execute_reply":"2023-09-18T15:52:43.571728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and analyze raw data","metadata":{"_uuid":"37655523-148f-449f-8f4c-c28169f35953","_cell_guid":"b34dc02c-ea6b-4556-b4c2-ff5622d82ec9","trusted":true}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/stockmarket-sentiment-dataset/stock_data.csv\")","metadata":{"_uuid":"64cac6e8-977f-4410-8dc9-6c397ef34b44","_cell_guid":"53c939bd-d367-479f-8eb3-aaaef57496b3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.574613Z","iopub.execute_input":"2023-09-18T15:52:43.574970Z","iopub.status.idle":"2023-09-18T15:52:43.614779Z","shell.execute_reply.started":"2023-09-18T15:52:43.574942Z","shell.execute_reply":"2023-09-18T15:52:43.613431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"_uuid":"a19c7d2c-c8c6-44b1-8f05-111dd3b1c4e7","_cell_guid":"e5af68a8-f339-4369-b247-1d9072a2dfb8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.616937Z","iopub.execute_input":"2023-09-18T15:52:43.617420Z","iopub.status.idle":"2023-09-18T15:52:43.636365Z","shell.execute_reply.started":"2023-09-18T15:52:43.617385Z","shell.execute_reply":"2023-09-18T15:52:43.635001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"_uuid":"6e15f612-caca-4a7f-af6d-29e381310421","_cell_guid":"7ca52150-8305-46c5-94d6-0c5b68d168e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.639535Z","iopub.execute_input":"2023-09-18T15:52:43.640757Z","iopub.status.idle":"2023-09-18T15:52:43.660693Z","shell.execute_reply.started":"2023-09-18T15:52:43.640698Z","shell.execute_reply":"2023-09-18T15:52:43.659329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"Sentiment\"].value_counts()","metadata":{"_uuid":"bd03fbdc-e073-43e4-9b1f-f1734b8bb139","_cell_guid":"32087dbf-6777-4325-9558-2b1e2dc96662","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.662313Z","iopub.execute_input":"2023-09-18T15:52:43.662733Z","iopub.status.idle":"2023-09-18T15:52:43.674699Z","shell.execute_reply.started":"2023-09-18T15:52:43.662684Z","shell.execute_reply":"2023-09-18T15:52:43.672960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_count = data[data['Sentiment'] == 1]['Sentiment'].count()\nnegative_count = data[data['Sentiment'] == -1]['Sentiment'].count()\n\npercentage_positive = (positive_count / data['Sentiment'].count() * 100).round()\npercentage_negative = (negative_count / data['Sentiment'].count() * 100).round()\n\nprint(f'Positive vs negative (%): {percentage_positive} - {percentage_negative}')","metadata":{"_uuid":"b864b819-2daf-40cc-a802-289e5a41d217","_cell_guid":"c419c5fd-9161-45b5-bd41-c4abc2646bca","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.676755Z","iopub.execute_input":"2023-09-18T15:52:43.677240Z","iopub.status.idle":"2023-09-18T15:52:43.691917Z","shell.execute_reply.started":"2023-09-18T15:52:43.677204Z","shell.execute_reply":"2023-09-18T15:52:43.689390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The data set contains 5791 examples:\n    - 3685 examples of a positive sentiment (Sentiment = 1)\n    - 2106 examples of a negative sentiment (Sentiment = -1)\n- The ratio of positive to negative examples is 64% - 36%. The class proportion is moderately skewed, so at this point I will not take additional actions to address this imbalance.\n- The machine learning task is a binary classification task.\n- Data is unstructured in the form of short texts. Features will be extracted from the available texts.","metadata":{}},{"cell_type":"markdown","source":"## Split data into training, validation and test sets\n- Examples will be split into:\n    - Training set - Examples used by machine learning algorithms to train models,\n    - Validation set - Examples used to validate the models, compare the models and select the best model,\n    - Test set - Examples used to report predictive performance of the final model.\n- Since the task is classification, I will apply stratified split to keep the same ratio of class labels in training, validation and test sets.\n","metadata":{"_uuid":"01a6e69b-ea7f-493a-a89a-671506d1985a","_cell_guid":"9d2862d9-df65-4d84-acd8-06b2a0d75dba","trusted":true}},{"cell_type":"code","source":"# First, split the data into training and remaining data (80% for training, 20% for the remaining)\nX_train, X_remaining, y_train, y_remaining = train_test_split(data['Text'], data['Sentiment'], test_size=0.2, stratify=data['Sentiment'], random_state=42)\n#data.drop(columns=['Text'])\n\n# Then, split the remaining data into validation and test sets (50% for each)\nX_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, stratify=y_remaining, random_state=42)","metadata":{"_uuid":"c636984f-6c9e-4057-9e88-c86ba3ce42ed","_cell_guid":"f325fe31-e412-45e9-a19b-e1635fb1e2b3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.695046Z","iopub.execute_input":"2023-09-18T15:52:43.695463Z","iopub.status.idle":"2023-09-18T15:52:43.715235Z","shell.execute_reply.started":"2023-09-18T15:52:43.695432Z","shell.execute_reply":"2023-09-18T15:52:43.713627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Training split ({X_train.shape[0]} examples):')\nprint(y_train.value_counts())","metadata":{"_uuid":"e73390d2-4d57-4f01-9717-d3cb0075a495","_cell_guid":"cf220851-dc01-451e-9f41-7a6ecc575693","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.718425Z","iopub.execute_input":"2023-09-18T15:52:43.718939Z","iopub.status.idle":"2023-09-18T15:52:43.726749Z","shell.execute_reply.started":"2023-09-18T15:52:43.718894Z","shell.execute_reply":"2023-09-18T15:52:43.725712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Validation split ({X_val.shape[0]} examples):')\nprint(y_val.value_counts())","metadata":{"_uuid":"88d71295-044a-4a71-bf4c-0c31995369fa","_cell_guid":"74fc1a6a-50f5-44b9-a689-293db8ad9a32","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.728695Z","iopub.execute_input":"2023-09-18T15:52:43.729178Z","iopub.status.idle":"2023-09-18T15:52:43.743626Z","shell.execute_reply.started":"2023-09-18T15:52:43.729141Z","shell.execute_reply":"2023-09-18T15:52:43.741971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Test split ({X_test.shape[0]} examples):')\nprint(y_test.value_counts())","metadata":{"_uuid":"15112375-bff5-4689-85ed-6c0db1e81331","_cell_guid":"9f557d6f-e757-4c6e-93df-4afda533f6f3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:52:43.745529Z","iopub.execute_input":"2023-09-18T15:52:43.745919Z","iopub.status.idle":"2023-09-18T15:52:43.758827Z","shell.execute_reply.started":"2023-09-18T15:52:43.745889Z","shell.execute_reply":"2023-09-18T15:52:43.756668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory data analysis\nIn this step, feature extraction and selection is performed, as well as analysis of words that frequently appear in texts of positive and negative sentiments.","metadata":{"_uuid":"44cc95d2-e8e2-410c-9ee4-7103dd03ad74","_cell_guid":"218377d5-003a-4387-ba1a-2245b54b94ec","trusted":true}},{"cell_type":"markdown","source":"### Feature extraction and selection\n- Features are tokens that appear in texts, e.g., words, numbers, punctionation marks.\n- To extract features:\n    1. Texts are tokenized\n    2. Porter stemmer is used to find common stems of words, so that different forms like buy and buying are treated as the same feature\n    3. Stop words and punctionation marks are removed\n    4. Tokens that have less than four characters are removed. In this manner domain names and stock tickers are removed. In addition, four character stock tickers are filtered out manually.\n    5. Tokens that do not appear in at least 10 texts are removed","metadata":{"_uuid":"659c3405-3461-47d4-b277-85e147f372fe","_cell_guid":"a6c4aae1-cb92-4781-af02-8293283b6d8c","trusted":true}},{"cell_type":"code","source":"nltk.download('stopwords')\nstop_words = set(stopwords.words(\"english\"))\nprint(stop_words)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:52:43.760970Z","iopub.execute_input":"2023-09-18T15:52:43.761773Z","iopub.status.idle":"2023-09-18T15:53:03.822115Z","shell.execute_reply.started":"2023-09-18T15:52:43.761734Z","shell.execute_reply":"2023-09-18T15:53:03.820450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuation = set(string.punctuation)\nprint(punctuation)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:03.826185Z","iopub.execute_input":"2023-09-18T15:53:03.826963Z","iopub.status.idle":"2023-09-18T15:53:03.834132Z","shell.execute_reply.started":"2023-09-18T15:53:03.826903Z","shell.execute_reply":"2023-09-18T15:53:03.832842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_tickers = set(['aapl', 'affi', 'amid', 'amzn', 'appl', 'bvsn', 'dndn', 'dvax', 'ebay', 'gevo', 'goog', 'imho', 'intc', 'invn', 'mani', 'morn', 'msft', 'nvda', 'pphm', 'qcom', 'swhc', 'yhoo', 'znga'])\nprint(stock_tickers)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:03.841603Z","iopub.execute_input":"2023-09-18T15:53:03.842274Z","iopub.status.idle":"2023-09-18T15:53:03.856986Z","shell.execute_reply.started":"2023-09-18T15:53:03.842237Z","shell.execute_reply":"2023-09-18T15:53:03.855416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer = PorterStemmer()\ndef custom_tokenizer(text):\n    tokens = text.split()\n    filtered_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words and token.isalpha() and token not in punctuation]\n    filtered_tokens = [token for token in filtered_tokens if token == 'buy' or (len(token) > 3 and token not in stock_tickers)]\n\n    return filtered_tokens","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:03.858926Z","iopub.execute_input":"2023-09-18T15:53:03.859422Z","iopub.status.idle":"2023-09-18T15:53:03.870897Z","shell.execute_reply.started":"2023-09-18T15:53:03.859380Z","shell.execute_reply":"2023-09-18T15:53:03.868860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vect = CountVectorizer(lowercase=True, min_df=10, tokenizer=custom_tokenizer)\nX_train_counts = count_vect.fit_transform(X_train)\nX_train_counts.shape","metadata":{"_uuid":"cfabfd6f-4f67-4720-b089-7054b47ac356","_cell_guid":"59d152a6-eab4-4c56-91e6-685559207949","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:53:03.873668Z","iopub.execute_input":"2023-09-18T15:53:03.874154Z","iopub.status.idle":"2023-09-18T15:53:05.328710Z","shell.execute_reply.started":"2023-09-18T15:53:03.874099Z","shell.execute_reply":"2023-09-18T15:53:05.326759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = count_vect.get_feature_names_out()\nprint(feature_names)","metadata":{"_uuid":"386f521d-b74b-478a-8806-00ad66fcd4e7","_cell_guid":"f2ee97e8-792f-4474-afbf-3a330efbc9e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:53:05.330506Z","iopub.execute_input":"2023-09-18T15:53:05.331450Z","iopub.status.idle":"2023-09-18T15:53:05.340093Z","shell.execute_reply.started":"2023-09-18T15:53:05.331398Z","shell.execute_reply":"2023-09-18T15:53:05.338794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Wordcloud of texts with positive sentiment","metadata":{}},{"cell_type":"code","source":"positive_sentiment_data = X_train_counts[np.where(y_train == 1)[0]]\n\n# Convert the sparse matrix to a dense array\npositive_sentiment_word_counts = np.asarray(positive_sentiment_data.sum(axis=0)).flatten()\n\n# Create a dictionary of words and their counts\npositive_sentiment_word_count_dict = dict(zip(feature_names, positive_sentiment_word_counts))\n\n# Print the sorted dictionary\nprint(', '.join([f'{key}: {value}' for key, value in dict(sorted(positive_sentiment_word_count_dict.items(), key=lambda item: item[1], reverse=True)).items()]))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:05.341480Z","iopub.execute_input":"2023-09-18T15:53:05.342076Z","iopub.status.idle":"2023-09-18T15:53:05.360496Z","shell.execute_reply.started":"2023-09-18T15:53:05.342005Z","shell.execute_reply":"2023-09-18T15:53:05.359088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(positive_sentiment_word_count_dict)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:05.362574Z","iopub.execute_input":"2023-09-18T15:53:05.364454Z","iopub.status.idle":"2023-09-18T15:53:06.406328Z","shell.execute_reply.started":"2023-09-18T15:53:05.364409Z","shell.execute_reply":"2023-09-18T15:53:06.405223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Wordcloud of texts with negative sentiment","metadata":{}},{"cell_type":"code","source":"negative_sentiment_data = X_train_counts[np.where(y_train == -1)[0]]\n\n# Convert the sparse matrix to a dense array\nnegative_sentiment_word_counts = np.asarray(negative_sentiment_data.sum(axis=0)).flatten()\n\n# Create a dictionary of words and their counts\nnegative_sentiment_word_count_dict = dict(zip(feature_names, negative_sentiment_word_counts))\n\n# Print the sorted dictionary\nprint(', '.join([f'{key}: {value}' for key, value in dict(sorted(negative_sentiment_word_count_dict.items(), key=lambda item: item[1], reverse=True)).items()]))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:06.407715Z","iopub.execute_input":"2023-09-18T15:53:06.408863Z","iopub.status.idle":"2023-09-18T15:53:06.421812Z","shell.execute_reply.started":"2023-09-18T15:53:06.408818Z","shell.execute_reply":"2023-09-18T15:53:06.420007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(negative_sentiment_word_count_dict)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:06.423427Z","iopub.execute_input":"2023-09-18T15:53:06.424553Z","iopub.status.idle":"2023-09-18T15:53:07.546564Z","shell.execute_reply.started":"2023-09-18T15:53:06.424502Z","shell.execute_reply":"2023-09-18T15:53:07.544913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results\n- After performing feature extraction and selection the data set contains 423 features representing content words from posts, excluding stock tickers and domain names.\n- Word frequencies and wordclouds are computed separately for training set posts of positive and negative sentiment to understand whether the features are meaningful.\n- As expected, words buy and long are more frequently mentioned in posts of positive sentiment, while words short and sell in posts of negative sentiment.\n- Interestingly, word coronavirus more frequently appears in posts of negative sentiment. ","metadata":{}},{"cell_type":"markdown","source":"## TF-IDF\n- CountVectorizer counts how many times each word appeared in a post. The drawback of counts is that they are not normalized, i.e., it is more likely that a word will appear more frequently in a longer post. To normalize feature values, I will apply TF-IDF.\n- **TF-IDF** stands for term frequency-inverse document frequency.\n- In this representation features are words that appear in posts. Words are referred to as terms.\n- Posts, i.e., short texts, are referred to as documents. A set of documents is referred to as corpus.\n- **Inverse document frequency (IDF)** is computed for each term as: log(the number of documents in a corpus / the number of documents that contain the term)\n- **Term frequency (TF)** is computed for a specific term and a document as: the number of times the term appeared in the document / total number of terms in that document\n- **TF-IDF (term, document)** = TF (term, document) * IDF (term)\n- IDF is computed once on the training set for all terms / features. When we want to compute TF-IDF for documents from validation and test sets, for each document we compute term frequencies and multiply them with IDFs computed on the training set.","metadata":{"_uuid":"7111ca0b-c784-49fb-9655-1daa483610ec","_cell_guid":"1d58baa2-5a35-4c48-8c44-272bb27f98bf","trusted":true}},{"cell_type":"markdown","source":"### Compute word occurances for validation and test set","metadata":{}},{"cell_type":"code","source":"X_train_counts.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:07.548431Z","iopub.execute_input":"2023-09-18T15:53:07.548803Z","iopub.status.idle":"2023-09-18T15:53:07.558277Z","shell.execute_reply.started":"2023-09-18T15:53:07.548773Z","shell.execute_reply":"2023-09-18T15:53:07.556285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_counts = count_vect.transform(X_val)\nX_val_counts.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:07.560077Z","iopub.execute_input":"2023-09-18T15:53:07.560591Z","iopub.status.idle":"2023-09-18T15:53:07.766619Z","shell.execute_reply.started":"2023-09-18T15:53:07.560540Z","shell.execute_reply":"2023-09-18T15:53:07.764661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_counts = count_vect.transform(X_test)\nX_test_counts.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:07.768740Z","iopub.execute_input":"2023-09-18T15:53:07.769292Z","iopub.status.idle":"2023-09-18T15:53:07.974976Z","shell.execute_reply.started":"2023-09-18T15:53:07.769251Z","shell.execute_reply":"2023-09-18T15:53:07.973962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert occurances to frequencies","metadata":{"_uuid":"9c0e03f5-f7e0-4b6e-8040-9cc6be6b84ad","_cell_guid":"f6c1494f-6320-42be-8297-6a60e8dc1061","trusted":true}},{"cell_type":"code","source":"tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape","metadata":{"_uuid":"c4470ad1-7558-48d4-a86e-0829f7b316f5","_cell_guid":"2263752a-3422-4eaa-b5ee-d5efbf6e5898","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-18T15:53:07.977512Z","iopub.execute_input":"2023-09-18T15:53:07.978140Z","iopub.status.idle":"2023-09-18T15:53:07.997028Z","shell.execute_reply.started":"2023-09-18T15:53:07.978082Z","shell.execute_reply":"2023-09-18T15:53:07.993896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_tfidf = tfidf_transformer.transform(X_val_counts)\nX_val_tfidf.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:07.999220Z","iopub.execute_input":"2023-09-18T15:53:08.001507Z","iopub.status.idle":"2023-09-18T15:53:08.023773Z","shell.execute_reply.started":"2023-09-18T15:53:08.001450Z","shell.execute_reply":"2023-09-18T15:53:08.021395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_tfidf = tfidf_transformer.transform(X_test_counts)\nX_test_tfidf.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.026218Z","iopub.execute_input":"2023-09-18T15:53:08.026802Z","iopub.status.idle":"2023-09-18T15:53:08.049533Z","shell.execute_reply.started":"2023-09-18T15:53:08.026759Z","shell.execute_reply":"2023-09-18T15:53:08.046254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfidf_dense = X_train_tfidf.toarray()\nprint(X_train_tfidf_dense[1])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.051886Z","iopub.execute_input":"2023-09-18T15:53:08.054450Z","iopub.status.idle":"2023-09-18T15:53:08.100734Z","shell.execute_reply.started":"2023-09-18T15:53:08.054275Z","shell.execute_reply":"2023-09-18T15:53:08.098551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute summary statistics\nmean_tfidf = np.mean(X_train_tfidf_dense)\nmedian_tfidf = np.median(X_train_tfidf_dense)\nstd_tfidf = np.std(X_train_tfidf_dense)\nmin_tfidf = np.min(X_train_tfidf_dense)\nmax_tfidf = np.max(X_train_tfidf_dense)\n\n# Print the summary statistics\nprint(\"Summary Statistics for TF-IDF Matrix:\")\nprint(f\"Mean: {mean_tfidf}\")\nprint(f\"Median: {median_tfidf}\")\nprint(f\"Standard Deviation: {std_tfidf}\")\nprint(f\"Minimum: {min_tfidf}\")\nprint(f\"Maximum: {max_tfidf}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.103434Z","iopub.execute_input":"2023-09-18T15:53:08.103990Z","iopub.status.idle":"2023-09-18T15:53:08.149259Z","shell.execute_reply.started":"2023-09-18T15:53:08.103945Z","shell.execute_reply":"2023-09-18T15:53:08.147837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Measuring predictive performance of a classification model\n- The performance_metrics function measures accuracy, precision, recall and F1-score of a classification model on training and validation sets.\n- The plot_confusion_matrix function plots confusion matrix for predicted and true labels.","metadata":{}},{"cell_type":"markdown","source":"### Performance metrics","metadata":{}},{"cell_type":"code","source":"def performance_metrics(y_train_true, y_train_pred, y_val_true, y_val_pred):\n    # Compute metrics for the training set\n    train_accuracy = accuracy_score(y_train_true, y_train_pred)\n    train_precision_positive = precision_score(y_train_true, y_train_pred, pos_label=1)\n    train_recall_positive = recall_score(y_train_true, y_train_pred, pos_label=1)\n    train_f1_positive = f1_score(y_train_true, y_train_pred, pos_label=1)\n    \n    train_precision_negative = precision_score(y_train_true, y_train_pred, pos_label=-1)\n    train_recall_negative = recall_score(y_train_true, y_train_pred, pos_label=-1)\n    train_f1_negative = f1_score(y_train_true, y_train_pred, pos_label=-1)\n\n    # Compute metrics for the validation set\n    val_accuracy = accuracy_score(y_val_true, y_val_pred)\n    val_precision_positive = precision_score(y_val_true, y_val_pred, pos_label=1)\n    val_recall_positive = recall_score(y_val_true, y_val_pred, pos_label=1)\n    val_f1_positive = f1_score(y_val_true, y_val_pred, pos_label=1)\n    \n    val_precision_negative = precision_score(y_val_true, y_val_pred, pos_label=-1)\n    val_recall_negative = recall_score(y_val_true, y_val_pred, pos_label=-1)\n    val_f1_negative = f1_score(y_val_true, y_val_pred, pos_label=-1)\n\n    # Print the metrics for the positive class\n    print(\"Positive Class Metrics:\")\n    print(\"Training Set:\")\n    print(f\"Accuracy: {train_accuracy:.2f}\")\n    print(f\"Precision: {train_precision_positive:.2f}\")\n    print(f\"Recall: {train_recall_positive:.2f}\")\n    print(f\"F1-Score: {train_f1_positive:.2f}\")\n    \n    print(\"\\nValidation Set:\")\n    print(f\"Accuracy: {val_accuracy:.2f}\")\n    print(f\"Precision: {val_precision_positive:.2f}\")\n    print(f\"Recall: {val_recall_positive:.2f}\")\n    print(f\"F1-Score: {val_f1_positive:.2f}\")\n\n    # Print the metrics for the negative class\n    print(\"\\nNegative Class Metrics:\")\n    print(\"Training Set:\")\n    print(f\"Accuracy: {train_accuracy:.2f}\")\n    print(f\"Precision: {train_precision_negative:.2f}\")\n    print(f\"Recall: {train_recall_negative:.2f}\")\n    print(f\"F1-Score: {train_f1_negative:.2f}\")\n    \n    print(\"\\nValidation Set:\")\n    print(f\"Accuracy: {val_accuracy:.2f}\")\n    print(f\"Precision: {val_precision_negative:.2f}\")\n    print(f\"Recall: {val_recall_negative:.2f}\")\n    print(f\"F1-Score: {val_f1_negative:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.150694Z","iopub.execute_input":"2023-09-18T15:53:08.151205Z","iopub.status.idle":"2023-09-18T15:53:08.168389Z","shell.execute_reply.started":"2023-09-18T15:53:08.151161Z","shell.execute_reply":"2023-09-18T15:53:08.166224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion matrix","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(true_labels, predicted_labels, title):\n    # Create a confusion matrix\n    cm = confusion_matrix(true_labels, predicted_labels)\n\n    # Plot the confusion matrix using seaborn and matplotlib\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n                xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n                yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title('Confusion Matrix' + (' - ' + title if len(title) > 0 else ''))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.170095Z","iopub.execute_input":"2023-09-18T15:53:08.170514Z","iopub.status.idle":"2023-09-18T15:53:08.195507Z","shell.execute_reply.started":"2023-09-18T15:53:08.170467Z","shell.execute_reply":"2023-09-18T15:53:08.193327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and validation of classification models\n- Function train_and_validate takes an instance of a classifier, trains and validates a model, prints performance metrics and confusion matrices.\n- Most frequent class model is used as a baseline. It predicts the most frequent class label in the training set (positive) for all examples in the validation set. It is a straightforward baseline that serves as a point of comparison for more sophisticated models. The idea is that any model that is trained performs better than this baseline. If the trained model performs worse than this baseline, typically there is a bug in the model that should be addressed.\n- The following models will be trained and validated:\n    - Naive Bayes\n    - SVM\n    - Gradient boosting\n    - Random forest","metadata":{}},{"cell_type":"code","source":"def train_and_validate(clf, X_train, y_train, X_val, y_val):\n    start_time = time.time()\n    clf.fit(X_train, y_train)\n    end_time = time.time()\n    training_time = end_time - start_time\n    print(f\"Training time: {training_time:.2f} seconds\\n\")\n    predictions_train = clf.predict(X_train)\n    predictions_val = clf.predict(X_val)\n    performance_metrics(y_train, predictions_train, y_val, predictions_val)\n    plot_confusion_matrix(y_train, predictions_train, 'Training set')\n    plot_confusion_matrix(y_val, predictions_val, 'Validation set')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.197953Z","iopub.execute_input":"2023-09-18T15:53:08.198529Z","iopub.status.idle":"2023-09-18T15:53:08.218609Z","shell.execute_reply.started":"2023-09-18T15:53:08.198476Z","shell.execute_reply":"2023-09-18T15:53:08.216633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Most frequent class baseline","metadata":{"_uuid":"742a24f3-6b14-4367-9ca8-9aa7bc530bb5","_cell_guid":"da3b32d9-c485-424b-bcf8-58ca2216fe0f","trusted":true}},{"cell_type":"code","source":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")\ntrain_and_validate(dummy_clf, X_train_tfidf, y_train, X_val_tfidf, y_val)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.220775Z","iopub.execute_input":"2023-09-18T15:53:08.221341Z","iopub.status.idle":"2023-09-18T15:53:08.784545Z","shell.execute_reply.started":"2023-09-18T15:53:08.221304Z","shell.execute_reply":"2023-09-18T15:53:08.782940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes","metadata":{"_uuid":"2834cd80-b4e4-4d33-a731-8fdedb1f48b7","_cell_guid":"40b2f399-191d-4b11-a388-e615b6ea2df4","trusted":true}},{"cell_type":"code","source":"nb_clf = MultinomialNB()\ntrain_and_validate(nb_clf, X_train_tfidf, y_train, X_val_tfidf, y_val)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:08.786675Z","iopub.execute_input":"2023-09-18T15:53:08.787697Z","iopub.status.idle":"2023-09-18T15:53:09.385142Z","shell.execute_reply.started":"2023-09-18T15:53:08.787642Z","shell.execute_reply":"2023-09-18T15:53:09.384132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Results\n- Naive Bayes model outperformed the most frequent class baseline in terms of:\n    - Accuracy measured on the validation set --> 72% Naive Bayes vs. 64% baseline\n    - F1 score measured on the validation set --> positive label: 80% Naive Bayes vs. 78% baseline - negative label: 51% Naive Bayes vs. 0% baseline\n- The confusion matrix reveals that there is a high number of false positives (126), i.e., post of negative sentiment, which are predicted as positive. There is more misclassified (126) than correctly classified (84) posts of negative sentiment. Naive Bayes model performs much better when classfying posts of positive sentiment --> 333 true positives and 36 false negatives. \n- This may happen because negative class label is underrepresented. Naive Bayes had more positive training examples and better learned to recognize positive sentiment. In the next step I will address class imbalance by giving the higher weight to negative examples.","metadata":{}},{"cell_type":"markdown","source":"### Random forest\n- Random forest algoritm has an option to assign different weights to examples of different class labels.\n- I will compare two versions of the random forest model, one that equaly weights examples of positive and negative class, and another with weights that simulate balanced data set.","metadata":{}},{"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\ntrain_and_validate(rf_clf, X_train_tfidf, y_train, X_val_tfidf, y_val)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:09.386570Z","iopub.execute_input":"2023-09-18T15:53:09.387463Z","iopub.status.idle":"2023-09-18T15:53:13.091487Z","shell.execute_reply.started":"2023-09-18T15:53:09.387424Z","shell.execute_reply":"2023-09-18T15:53:13.089948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_labels = [-1, 1]\nclass_weights = compute_class_weight('balanced', classes=class_labels, y=y_train)\nclass_weight_dict = {class_labels[i]: weight for i, weight in enumerate(class_weights)}\nprint(f'Negative class weight: {class_weights[0]}\\nPositive class weight: {class_weights[1]}')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:13.093444Z","iopub.execute_input":"2023-09-18T15:53:13.093855Z","iopub.status.idle":"2023-09-18T15:53:13.108191Z","shell.execute_reply.started":"2023-09-18T15:53:13.093825Z","shell.execute_reply":"2023-09-18T15:53:13.106145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_weight_clf = RandomForestClassifier(n_estimators=100, class_weight=class_weight_dict, random_state=42)\ntrain_and_validate(rf_weight_clf, X_train_tfidf, y_train, X_val_tfidf, y_val)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:53:13.110577Z","iopub.execute_input":"2023-09-18T15:53:13.113279Z","iopub.status.idle":"2023-09-18T15:53:16.559977Z","shell.execute_reply.started":"2023-09-18T15:53:13.113209Z","shell.execute_reply":"2023-09-18T15:53:16.559019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Results\n- Both versions of Random forest model outperformed the most frequent class baseline in terms of:\n    - Accuracy measured on the validation set --> 74% and 73% Random forest vs. 64% baseline\n    - F1 score measured on the validation set --> positive label: 81% both Random forest models vs. 78% baseline - negative label: 58% and 57% Random forest models vs. 0% baseline\n- The results of comparison of the Random forest model that equaly weights positive and negative examples, and Naive Bayes model, indicate the highest improvement in recall on negative class label --> 50% recall of Random forest vs. 40% recall of Naive Bayes. Other performance metrics are also slightly improved for Random forest. Therefore, Random forest algoritm is a better choice than Naive Bayes for this problem.\n- By changing weights of the class labels to simulate balanced training set, no additional improvement in performance of Random forest model is achieved.\n- Since class imbalance is eliminated as a cause of poor performance on negative class labels, another option is to expand feture set with features that can better capture negative sentiment.","metadata":{}}]}